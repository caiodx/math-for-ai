import { Topic } from '../types'

export const topics: Topic[] = [
  {
    id: 'algebra-linear',
    title: '√Ålgebra Linear',
    description: 'Vetores, matrizes e opera√ß√µes fundamentais',
    icon: 'üìê',
    color: 'bg-blue-500',
    lessons: [
      {
        id: 'vetores',
        title: 'Vetores',
        description: 'Entenda o que s√£o vetores e como trabalhar com eles',
        content: {
          theory: `
# Vetores

## O que √© um Vetor?

Imagine que voc√™ est√° dando dire√ß√µes para algu√©m chegar a um ponto no mapa. Voc√™ precisa dizer:
- **Quanto andar** (magnitude)
- **Para onde ir** (dire√ß√£o)

Um **vetor** √© exatamente isso: uma quantidade que possui tanto **magnitude** (tamanho) quanto **dire√ß√£o**.

### Analogia do Dia a Dia

Pense em um vetor como uma **seta**:
- O **comprimento da seta** = magnitude (qu√£o longe)
- A **dire√ß√£o da seta** = dire√ß√£o (para onde)

## Por que Vetores s√£o Importantes em IA?

Em Intelig√™ncia Artificial, vetores s√£o a **linguagem fundamental** dos dados:

- **Imagens**: Uma foto 28√ó28 pixels vira um vetor de 784 n√∫meros
- **Textos**: Palavras s√£o convertidas em vetores (embeddings)
- **Dados de usu√°rios**: Perfis s√£o representados como vetores
- **Redes Neurais**: Processam vetores em cada camada

### Exemplo Pr√°tico

Quando voc√™ faz uma busca no Google, seu texto vira um vetor, que √© comparado com outros vetores para encontrar resultados similares!

## Caracter√≠sticas dos Vetores

### 1. Dimens√£o
O n√∫mero de componentes que o vetor possui:
- **2D**: [x, y] - como coordenadas no plano
- **3D**: [x, y, z] - como coordenadas no espa√ßo
- **nD**: [v‚ÇÅ, v‚ÇÇ, ..., v‚Çô] - vetores de alta dimens√£o (comuns em IA)

### 2. Magnitude (Comprimento)
O tamanho do vetor, calculado usando o teorema de Pit√°goras:
- Para [3, 4]: |v| = ‚àö(3¬≤ + 4¬≤) = ‚àö25 = 5

### 3. Dire√ß√£o
Para onde o vetor aponta no espa√ßo

## Representa√ß√£o Visual

Veja os diagramas abaixo para visualizar vetores no plano 2D. Cada vetor √© mostrado como uma seta que parte da origem (0, 0) at√© o ponto final.

## Opera√ß√µes B√°sicas

### 1. Soma de Vetores

Para somar dois vetores, voc√™ **some cada componente**:

**v** = [1, 2] + **u** = [3, 4] = [4, 6]

**Interpreta√ß√£o Visual**: Se voc√™ andar na dire√ß√£o do vetor v e depois na dire√ß√£o do vetor u, o resultado √© como se voc√™ tivesse andado diretamente na dire√ß√£o do vetor soma!

### 2. Multiplica√ß√£o por Escalar

Multiplique cada componente pelo n√∫mero:

2 √ó [1, 2, 3] = [2, 4, 6]

**Interpreta√ß√£o**: Voc√™ est√° **esticando** ou **encolhendo** o vetor, mantendo a mesma dire√ß√£o.

- Se o escalar > 1: vetor fica maior
- Se 0 < escalar < 1: vetor fica menor
- Se escalar < 0: vetor inverte de dire√ß√£o

### 3. Produto Escalar (Dot Product)

Soma dos produtos dos componentes correspondentes:

[1, 2] ¬∑ [3, 4] = (1√ó3) + (2√ó4) = 3 + 8 = 11

**Interpreta√ß√£o**: O produto escalar mede:
- **Qu√£o alinhados** dois vetores est√£o
- Se o resultado √© **positivo**: vetores apontam em dire√ß√µes similares
- Se o resultado √© **negativo**: vetores apontam em dire√ß√µes opostas
- Se o resultado √© **zero**: vetores s√£o perpendiculares

**Em IA**: O produto escalar √© usado para medir **similaridade** entre vetores de dados!
          `,
          examples: [
            {
              title: 'Exemplo 1: Soma de Vetores',
              description: 'Some os vetores [2, 5] e [3, 1]',
              solution: '[2, 5] + [3, 1] = [2+3, 5+1] = [5, 6]',
            },
            {
              title: 'Exemplo 2: Produto Escalar',
              description: 'Calcule o produto escalar de [1, 3] e [4, 2]',
              solution: '[1, 3] ¬∑ [4, 2] = (1√ó4) + (3√ó2) = 4 + 6 = 10',
            },
          ],
          visualizations: [
            {
              type: 'interactive',
              component: 'vector',
              data: { showSum: true, showDotProduct: true },
            },
          ],
        },
        exercises: [
          {
            id: 'ex1',
            question: 'Qual √© o resultado da soma [3, 7] + [2, 1]?',
            type: 'multiple-choice',
            options: ['[5, 8]', '[6, 7]', '[3, 8]', '[5, 7]'],
            correctAnswer: '[5, 8]',
            explanation: 'Some cada componente: 3+2=5 e 7+1=8',
          },
          {
            id: 'ex2',
            question: 'Calcule o produto escalar: [2, 4] ¬∑ [3, 1]',
            type: 'input',
            correctAnswer: '10',
            explanation: '(2√ó3) + (4√ó1) = 6 + 4 = 10',
            hint: 'Multiplique os componentes correspondentes e some',
          },
          {
            id: 'ex3',
            question: 'Qual √© a magnitude (comprimento) aproximado do vetor [3, 4]?',
            type: 'multiple-choice',
            options: ['5', '7', '12', '25'],
            correctAnswer: '5',
            explanation: 'Magnitude = ‚àö(3¬≤ + 4¬≤) = ‚àö(9 + 16) = ‚àö25 = 5',
            hint: 'Use o teorema de Pit√°goras: ‚àö(x¬≤ + y¬≤)',
          },
          {
            id: 'ex4',
            question: 'Se v = [1, 2] e u = [3, 1], qual √© v - u?',
            type: 'input',
            correctAnswer: '[-2, 1]',
            explanation: '[1, 2] - [3, 1] = [1-3, 2-1] = [-2, 1]',
          },
        ],
      },
      {
        id: 'magnitude-direcao',
        title: 'Magnitude e Dire√ß√£o de Vetores',
        description: 'Aprenda a calcular o comprimento e dire√ß√£o de vetores',
        content: {
          theory: `
# Magnitude e Dire√ß√£o

## Magnitude (Comprimento)

A magnitude de um vetor **v** = [x, y] √© calculada usando o teorema de Pit√°goras:

**|v| = ‚àö(x¬≤ + y¬≤)**

Para vetores 3D: **|v| = ‚àö(x¬≤ + y¬≤ + z¬≤)**

## Dire√ß√£o

A dire√ß√£o de um vetor √© o √¢ngulo que ele forma com o eixo X positivo:

**Œ∏ = arctan(y/x)**

## Vetor Unit√°rio

Um vetor unit√°rio tem magnitude 1. Para normalizar um vetor:

**√ª = v / |v|**

## Aplica√ß√£o em IA

- Normaliza√ß√£o de features (importante para ML)
- C√°lculo de similaridade entre vetores
- Dist√¢ncias em espa√ßos de alta dimens√£o
          `,
          examples: [
            {
              title: 'Exemplo: Magnitude',
              description: 'Calcule a magnitude de [3, 4]',
              solution: '|v| = ‚àö(3¬≤ + 4¬≤) = ‚àö(9 + 16) = ‚àö25 = 5',
            },
            {
              title: 'Exemplo: Vetor Unit√°rio',
              description: 'Normalize o vetor [3, 4]',
              solution: '|v| = 5, ent√£o √ª = [3/5, 4/5] = [0.6, 0.8]',
            },
          ],
        },
        exercises: [
          {
            id: 'ex1',
            question: 'Qual √© a magnitude de [5, 12]?',
            type: 'input',
            correctAnswer: '13',
            explanation: '‚àö(5¬≤ + 12¬≤) = ‚àö(25 + 144) = ‚àö169 = 13',
          },
          {
            id: 'ex2',
            question: 'Um vetor unit√°rio tem magnitude:',
            type: 'multiple-choice',
            options: ['1', '0', '2', 'Depende do vetor'],
            correctAnswer: '1',
            explanation: 'Por defini√ß√£o, um vetor unit√°rio sempre tem magnitude 1',
          },
        ],
      },
      {
        id: 'matrizes',
        title: 'Matrizes',
        description: 'Aprenda sobre matrizes e suas opera√ß√µes',
        content: {
          theory: `
# Matrizes

## O que √© uma Matriz?

Imagine uma **tabela de Excel** ou uma **planilha** com n√∫meros organizados em linhas e colunas. Isso √© uma matriz!

Uma **matriz** √© uma estrutura retangular de n√∫meros, onde cada n√∫mero tem uma posi√ß√£o espec√≠fica (linha, coluna).

### Analogia do Dia a Dia

- **Planilha Excel**: Cada c√©lula tem uma posi√ß√£o (A1, B2, etc.)
- **Tabela de dados**: Linhas = registros, Colunas = caracter√≠sticas
- **Imagem digital**: Uma foto √© uma matriz de pixels (cada pixel tem cor e posi√ß√£o)

## Por que Matrizes s√£o Fundamentais em IA?

### 1. Representa√ß√£o de Dados
- **Imagens**: Uma foto 28√ó28 vira uma matriz 28√ó28 de valores de pixels
- **Dados tabulares**: Cada linha √© um exemplo, cada coluna √© uma feature
- **Textos**: Embeddings s√£o organizados como matrizes

### 2. Transforma√ß√µes
Matrizes podem **transformar** dados:
- **Rota√ß√£o**: Girar uma imagem
- **Escala**: Aumentar/diminuir
- **Proje√ß√£o**: Reduzir dimens√µes (PCA)

### 3. Redes Neurais
Cada **camada** de uma rede neural √© uma multiplica√ß√£o de matrizes:
- Entrada (vetor) √ó Pesos (matriz) = Sa√≠da (vetor)

## Representa√ß√£o Visual

Veja o diagrama abaixo para visualizar uma matriz e sua transposta. A transposta "vira" a matriz, trocando linhas por colunas.

## Opera√ß√µes B√°sicas

### 1. Multiplica√ß√£o por Escalar

Multiplique **cada elemento** pelo n√∫mero:

2 √ó [[1, 2], [3, 4]] = [[2, 4], [6, 8]]

**Interpreta√ß√£o**: Voc√™ est√° **escalando** toda a matriz, como aumentar o brilho de uma imagem.

### 2. Soma de Matrizes

Some **elemento por elemento** (matrizes devem ter as mesmas dimens√µes):

[[1, 2], [3, 4]] + [[5, 6], [7, 8]] = [[6, 8], [10, 12]]

### 3. Multiplica√ß√£o de Matrizes

Para multiplicar A √ó B:
- **Regra**: N√∫mero de colunas de A = N√∫mero de linhas de B
- **Resultado**: Matriz com linhas de A e colunas de B
- **C√°lculo**: Cada elemento √© o produto escalar de uma linha de A por uma coluna de B

**Exemplo**:
[[1, 2], [3, 4]] √ó [[5, 6], [7, 8]] = [[19, 22], [43, 50]]

**Em IA**: A multiplica√ß√£o de matrizes √© o cora√ß√£o das redes neurais!

### 4. Transposta

A transposta A·µÄ "vira" a matriz:
- Linhas viram colunas
- Colunas viram linhas

**Exemplo**: Se A = [[1, 2, 3], [4, 5, 6]], ent√£o A·µÄ = [[1, 4], [2, 5], [3, 6]]

Veja o diagrama visual abaixo para entender melhor!
          `,
          examples: [
            {
              title: 'Exemplo: Multiplica√ß√£o por Escalar',
              description: 'Multiplique a matriz [[1, 2], [3, 4]] por 3',
              solution: '3 √ó [[1, 2], [3, 4]] = [[3, 6], [9, 12]]',
            },
          ],
          visualizations: [
            {
              type: 'interactive',
              component: 'matrix',
              data: { showOperations: true },
            },
          ],
        },
        exercises: [
          {
            id: 'ex1',
            question: 'Qual √© o resultado de 2 √ó [[1, 3], [2, 4]]?',
            type: 'multiple-choice',
            options: [
              '[[2, 6], [4, 8]]',
              '[[3, 5], [4, 6]]',
              '[[1, 3], [4, 8]]',
              '[[2, 3], [4, 8]]',
            ],
            correctAnswer: '[[2, 6], [4, 8]]',
            explanation: 'Multiplique cada elemento por 2',
          },
          {
            id: 'ex2',
            question: 'Qual √© a transposta da matriz [[1, 2], [3, 4]]?',
            type: 'multiple-choice',
            options: [
              '[[1, 3], [2, 4]]',
              '[[1, 2], [3, 4]]',
              '[[4, 3], [2, 1]]',
              '[[1, 4], [2, 3]]',
            ],
            correctAnswer: '[[1, 3], [2, 4]]',
            explanation: 'A transposta troca linhas por colunas',
          },
        ],
      },
    ],
  },
  {
    id: 'calculo',
    title: 'C√°lculo',
    description: 'Derivadas, gradientes e otimiza√ß√£o',
    icon: 'üìä',
    color: 'bg-green-500',
    lessons: [
      {
        id: 'limites',
        title: 'Limites',
        description: 'Entenda o conceito de limite, base fundamental para derivadas',
        content: {
          theory: `
# Limites

## O que √© um Limite?

Imagine que voc√™ est√° **se aproximando** de um ponto, mas **nunca chega exatamente nele**. O limite √© o valor que a fun√ß√£o "tende" quando voc√™ se aproxima infinitamente perto.

### Analogia do Dia a Dia

Pense em uma **parede**:
- Voc√™ pode se aproximar **muito, muito perto** da parede
- Mas nunca pode **atravessar** ela
- O limite √© como perguntar: "Se eu pudesse chegar infinitamente perto, qual seria o valor?"

## Por que Limites s√£o Importantes?

**Limites s√£o a base das derivadas!** Sem entender limites, voc√™ n√£o consegue entender derivadas de verdade.

### Defini√ß√£o Formal (Simplificada)

O limite de f(x) quando x se aproxima de a √© L:

**lim(x‚Üía) f(x) = L**

Isso significa: quando x fica **muito pr√≥ximo** de a, f(x) fica **muito pr√≥ximo** de L.

## Exemplos Pr√°ticos

### Exemplo 1: Limite Simples

lim(x‚Üí2) x = 2

**Por qu√™?** Quando x se aproxima de 2, x √© 2. Simples!

### Exemplo 2: Limite de Fun√ß√£o

lim(x‚Üí3) (x + 1) = 4

**Por qu√™?** Quando x se aproxima de 3, (x + 1) se aproxima de 4.

### Exemplo 3: Limite "Problem√°tico"

lim(x‚Üí0) (x¬≤ / x) = ?

Quando x = 0, temos 0/0 (indeterminado!). Mas podemos simplificar:
- x¬≤ / x = x (quando x ‚â† 0)
- Ent√£o lim(x‚Üí0) x = 0

## Limites e Derivadas

A derivada √© definida usando limites:

**f'(x) = lim(h‚Üí0) [f(x+h) - f(x)] / h**

Isso significa: a derivada √© o limite da **taxa de varia√ß√£o** quando a diferen√ßa h fica **infinitamente pequena**!

## Em C√≥digo

Em Python, voc√™ pode calcular limites numericamente (aproximando):

\`\`\`python
import numpy as np

# Limite de (x¬≤ - 1) / (x - 1) quando x ‚Üí 1
x_values = np.array([0.9, 0.99, 0.999, 0.9999])
f_values = (x_values**2 - 1) / (x_values - 1)
# Resultado se aproxima de 2!
\`\`\`

## Por que Isso Importa para IA?

- **Derivadas** (usadas em gradiente descendente) s√£o definidas com limites
- **Otimiza√ß√£o** precisa entender comportamento quando valores se aproximam de pontos cr√≠ticos
- **Backpropagation** calcula derivadas, que dependem de limites
          `,
          examples: [
            {
              title: 'Exemplo 1: Limite B√°sico',
              description: 'Calcule lim(x‚Üí5) (2x + 3)',
              solution: 'lim(x‚Üí5) (2x + 3) = 2(5) + 3 = 13',
            },
            {
              title: 'Exemplo 2: Limite com Simplifica√ß√£o',
              description: 'Calcule lim(x‚Üí2) (x¬≤ - 4) / (x - 2)',
              solution: 'Simplificando: (x-2)(x+2)/(x-2) = x+2. Ent√£o lim(x‚Üí2) (x+2) = 4',
            },
          ],
        },
        exercises: [
          {
            id: 'ex1',
            question: 'Qual √© lim(x‚Üí3) (x + 2)?',
            type: 'multiple-choice',
            options: ['3', '5', '6', '9'],
            correctAnswer: '5',
            explanation: 'Quando x se aproxima de 3, (x + 2) se aproxima de 5',
          },
          {
            id: 'ex2',
            question: 'Qual √© lim(x‚Üí0) (x¬≤ / x)?',
            type: 'input',
            correctAnswer: '0',
            explanation: 'Simplificando: x¬≤/x = x (quando x‚â†0). Ent√£o lim(x‚Üí0) x = 0',
          },
          {
            id: 'ex3',
            question: 'A derivada √© definida usando limites. Verdadeiro ou Falso?',
            type: 'multiple-choice',
            options: ['Verdadeiro', 'Falso'],
            correctAnswer: 'Verdadeiro',
            explanation: 'Sim! f\'(x) = lim(h‚Üí0) [f(x+h) - f(x)] / h',
          },
        ],
      },
      {
        id: 'derivadas',
        title: 'Derivadas',
        description: 'Entenda o conceito de derivada e sua import√¢ncia em IA',
        content: {
          theory: `
# Derivadas

## O que √© uma Derivada?

Imagine que voc√™ est√° **dirigindo um carro**. A derivada √© como o **veloc√≠metro**:
- Ela te diz **qu√£o r√°pido** voc√™ est√° mudando de posi√ß√£o
- Se voc√™ acelera, a derivada aumenta
- Se voc√™ freia, a derivada diminui

A **derivada** mede a **taxa de varia√ß√£o** de uma fun√ß√£o - ou seja, **qu√£o r√°pido** a fun√ß√£o est√° mudando em um ponto espec√≠fico.

## Conceito Visual

Veja o diagrama abaixo! A derivada em um ponto √© a **inclina√ß√£o da reta tangente** (a linha verde) que "toca" a curva naquele ponto.

### Interpreta√ß√£o Geom√©trica

- **Derivada positiva**: Fun√ß√£o est√° **subindo** (inclina√ß√£o para cima)
- **Derivada negativa**: Fun√ß√£o est√° **descendo** (inclina√ß√£o para baixo)
- **Derivada zero**: Fun√ß√£o est√° em um **pico ou vale** (m√°ximo ou m√≠nimo)

## Nota√ß√£o

- **f'(x)**: Derivada de f em rela√ß√£o a x (nota√ß√£o de Lagrange)
- **df/dx**: Derivada de f em rela√ß√£o a x (nota√ß√£o de Leibniz)
- **dy/dx**: Derivada de y em rela√ß√£o a x

## Regras B√°sicas (Voc√™ Precisa Saber!)

### 1. Regra da Pot√™ncia
Se f(x) = x‚Åø, ent√£o f'(x) = n¬∑x‚Åø‚Åª¬π

**Exemplos**:
- f(x) = x¬≤ ‚Üí f'(x) = 2x
- f(x) = x¬≥ ‚Üí f'(x) = 3x¬≤
- f(x) = x‚Åµ ‚Üí f'(x) = 5x‚Å¥

### 2. Regra da Constante
Se f(x) = c (constante), ent√£o f'(x) = 0

**Por qu√™?** Uma constante n√£o muda, ent√£o sua taxa de varia√ß√£o √© zero!

### 3. Regra da Soma
A derivada da soma √© a soma das derivadas:
(f + g)' = f' + g'

**Exemplo**: Se f(x) = x¬≤ + 3x, ent√£o f'(x) = 2x + 3

### 4. Regra do Produto (B√¥nus)
(f ¬∑ g)' = f'¬∑g + f¬∑g'

## Por que Derivadas s√£o ESSENCIAIS em IA?

### Gradiente Descendente

No treinamento de redes neurais, usamos derivadas para **otimizar**:

1. **Calculamos a derivada** da fun√ß√£o de erro em rela√ß√£o aos par√¢metros
2. **Ajustamos os par√¢metros** na dire√ß√£o oposta ao gradiente (descendo)
3. **Repetimos** at√© encontrar o m√≠nimo (menor erro poss√≠vel)

### Analogia da Montanha

Imagine que voc√™ est√° numa **montanha** (fun√ß√£o de erro) e quer chegar ao **vale** (m√≠nimo):
- A **derivada** te diz para onde a montanha est√° mais √≠ngreme
- Voc√™ anda na **dire√ß√£o oposta** (descendo)
- Eventualmente chega ao fundo do vale (m√≠nimo global)

### Backpropagation

O algoritmo de **backpropagation** (usado em todas as redes neurais) √© essencialmente calcular derivadas em cadeia!

Veja o diagrama visual abaixo para entender melhor a derivada como inclina√ß√£o!
          `,
          examples: [
            {
              title: 'Exemplo 1: Derivada de x¬≤',
              description: 'Calcule a derivada de f(x) = x¬≤',
              solution: 'f\'(x) = 2x (usando a regra da pot√™ncia: n=2, ent√£o 2¬∑x¬≤‚Åª¬π = 2x)',
            },
            {
              title: 'Exemplo 2: Derivada de 3x + 5',
              description: 'Calcule a derivada de f(x) = 3x + 5',
              solution: 'f\'(x) = 3 (derivada de 3x √© 3, derivada de constante 5 √© 0)',
            },
          ],
        },
        exercises: [
          {
            id: 'ex1',
            question: 'Qual √© a derivada de f(x) = x¬≥?',
            type: 'multiple-choice',
            options: ['3x¬≤', 'x¬≤', '3x', 'x¬≥'],
            correctAnswer: '3x¬≤',
            explanation: 'Regra da pot√™ncia: n=3, ent√£o 3¬∑x¬≥‚Åª¬π = 3x¬≤',
          },
          {
            id: 'ex2',
            question: 'Qual √© a derivada de f(x) = 5x?',
            type: 'input',
            correctAnswer: '5',
            explanation: 'A derivada de 5x √© 5 (constante vezes x)',
          },
          {
            id: 'ex3',
            question: 'Qual √© a derivada de f(x) = x¬≤ + 3x?',
            type: 'input',
            correctAnswer: '2x + 3',
            explanation: 'f\'(x) = 2x + 3 (derivada de x¬≤ √© 2x, derivada de 3x √© 3)',
          },
        ],
      },
      {
        id: 'gradientes',
        title: 'Gradientes',
        description: 'Entenda gradientes e como s√£o usados em otimiza√ß√£o',
        content: {
          theory: `
# Gradientes

## O que √© um Gradiente?

Se a **derivada** √© para fun√ß√µes de **uma vari√°vel**, o **gradiente** √© para fun√ß√µes de **m√∫ltiplas vari√°veis**.

O **gradiente** √© um **vetor** que cont√©m todas as derivadas parciais de uma fun√ß√£o. Ele aponta na dire√ß√£o de **maior crescimento** da fun√ß√£o.

### Analogia Visual

Imagine que voc√™ est√° numa **montanha**:
- O **gradiente** √© como uma **b√∫ssola** que aponta para o topo mais pr√≥ximo
- Se voc√™ quer **subir** mais r√°pido, siga o gradiente
- Se voc√™ quer **descer** (encontrar o m√≠nimo), v√° na dire√ß√£o **oposta**

## Defini√ß√£o Matem√°tica

Para uma fun√ß√£o f(x, y), o gradiente √©:

**‚àáf = [‚àÇf/‚àÇx, ‚àÇf/‚àÇy]**

Onde:
- **‚àÇf/‚àÇx**: Derivada parcial em rela√ß√£o a x (mantendo y constante)
- **‚àÇf/‚àÇy**: Derivada parcial em rela√ß√£o a y (mantendo x constante)

### Exemplo

Se f(x, y) = x¬≤ + y¬≤:
- ‚àÇf/‚àÇx = 2x (derivada de x¬≤, y¬≤ √© constante)
- ‚àÇf/‚àÇy = 2y (derivada de y¬≤, x¬≤ √© constante)
- **‚àáf = [2x, 2y]**

## Gradiente Descendente: O Algoritmo que Treina Redes Neurais

O **Gradient Descent** (Gradiente Descendente) √© o algoritmo **mais importante** em Machine Learning!

### Como Funciona

1. **Inicializa√ß√£o**: Come√ßamos com par√¢metros aleat√≥rios (pesos da rede)
2. **Forward Pass**: Calculamos a sa√≠da e o erro
3. **Backward Pass**: Calculamos o gradiente (usando backpropagation)
4. **Atualiza√ß√£o**: Movemos os par√¢metros na dire√ß√£o oposta ao gradiente
5. **Repeti√ß√£o**: Voltamos ao passo 2 at√© convergir

### F√≥rmula de Atualiza√ß√£o

**Œ∏ = Œ∏ - Œ± ¬∑ ‚àáf(Œ∏)**

Onde:
- **Œ∏**: Par√¢metros (pesos)
- **Œ±**: Taxa de aprendizado (learning rate)
- **‚àáf(Œ∏)**: Gradiente da fun√ß√£o de erro

### Learning Rate (Taxa de Aprendizado)

- **Muito pequeno**: Converg√™ncia lenta
- **Muito grande**: Pode "pular" o m√≠nimo
- **Ideal**: Ajustado experimentalmente

## Por que √© T√£o Importante?

### 1. Treinamento de Redes Neurais
**TODAS** as redes neurais s√£o treinadas com gradiente descendente (ou varia√ß√µes):
- Perceptron
- MLPs
- CNNs
- RNNs
- Transformers

### 2. Otimiza√ß√£o de Modelos
Qualquer modelo de ML que precisa "aprender" usa gradientes:
- Regress√£o Linear
- SVM
- √Årvores de Decis√£o (com boosting)

### 3. Fine-tuning
At√© mesmo modelos pr√©-treinados (como GPT, BERT) usam gradientes para ajuste fino!

## Varia√ß√µes Modernas

- **SGD** (Stochastic Gradient Descent): Usa um subconjunto dos dados
- **Adam**: Adapta a taxa de aprendizado automaticamente
- **RMSprop**: Otimiza√ß√£o para redes profundas

O gradiente descendente √© literalmente o **cora√ß√£o** do aprendizado de m√°quina moderno!
          `,
          examples: [
            {
              title: 'Exemplo: Gradiente de f(x,y) = x¬≤ + y¬≤',
              description: 'Calcule o gradiente',
              solution: '‚àáf = [2x, 2y] (derivada parcial em x √© 2x, em y √© 2y)',
            },
          ],
        },
        exercises: [
          {
            id: 'ex1',
            question: 'Para f(x,y) = 3x + 2y, qual √© o gradiente no ponto (1, 1)?',
            type: 'multiple-choice',
            options: ['[3, 2]', '[1, 1]', '[4, 3]', '[3, 1]'],
            correctAnswer: '[3, 2]',
            explanation: 'O gradiente √© sempre [3, 2] independente do ponto (fun√ß√£o linear)',
          },
          {
            id: 'ex2',
            question: 'No gradiente descendente, movemos na dire√ß√£o:',
            type: 'multiple-choice',
            options: [
              'Oposta ao gradiente',
              'Do gradiente',
              'Perpendicular ao gradiente',
              'Aleat√≥ria',
            ],
            correctAnswer: 'Oposta ao gradiente',
            explanation: 'Movemos na dire√ß√£o oposta para minimizar a fun√ß√£o',
          },
        ],
      },
    ],
  },
  {
    id: 'estatistica',
    title: 'Estat√≠stica',
    description: 'Distribui√ß√µes, correla√ß√£o e medidas estat√≠sticas',
    icon: 'üìà',
    color: 'bg-purple-500',
    lessons: [
      {
        id: 'medidas-tendencia',
        title: 'Medidas de Tend√™ncia Central',
        description: 'M√©dia, mediana e moda',
        content: {
          theory: `
# Medidas de Tend√™ncia Central

## O que s√£o Medidas de Tend√™ncia Central?

Imagine que voc√™ quer saber a **altura m√©dia** de uma turma. Ou o **sal√°rio t√≠pico** de uma empresa. Essas s√£o medidas de tend√™ncia central - elas descrevem o **"centro"** ou valor **t√≠pico** de um conjunto de dados.

## As Tr√™s Medidas Principais

### 1. M√©dia (Mean) - A Mais Comum

A m√©dia √© a **soma de todos os valores dividida pelo n√∫mero de valores**:

**M√©dia = (x‚ÇÅ + x‚ÇÇ + ... + x‚Çô) / n**

#### Exemplo Pr√°tico
Alturas: [160, 165, 170, 175, 180] cm
M√©dia = (160 + 165 + 170 + 175 + 180) / 5 = 170 cm

#### Caracter√≠sticas
- ‚úÖ F√°cil de calcular
- ‚úÖ Usa todos os dados
- ‚ùå Sens√≠vel a outliers (valores extremos)

#### Em IA
A m√©dia √© usada para:
- **Normaliza√ß√£o**: Subtrair a m√©dia dos dados
- **Feature engineering**: Criar features baseadas na m√©dia
- **Avalia√ß√£o**: Calcular m√©tricas como MAE (Mean Absolute Error)

### 2. Mediana (Median) - A Mais Robusta

A mediana √© o **valor do meio** quando os dados est√£o ordenados:
- Se n √© **√≠mpar**: valor central
- Se n √© **par**: m√©dia dos dois valores centrais

#### Exemplo Pr√°tico
Sal√°rios: [1000, 2000, 3000, 4000, 50000] (ordenados)
Mediana = 3000 (valor do meio)

**Por que usar mediana?** O sal√°rio de 50000 √© um outlier que distorce a m√©dia!

#### Caracter√≠sticas
- ‚úÖ **Robusta** a outliers
- ‚úÖ Representa melhor dados assim√©tricos
- ‚ùå N√£o usa todos os dados igualmente

#### Em IA
A mediana √© usada para:
- **Preprocessing**: Remover outliers
- **An√°lise explorat√≥ria**: Entender distribui√ß√µes
- **M√©tricas robustas**: Median Absolute Error

### 3. Moda (Mode) - O Mais Frequente

A moda √© o valor que **aparece com mais frequ√™ncia**.

#### Exemplo Pr√°tico
Cores favoritas: [azul, azul, verde, azul, vermelho, azul]
Moda = azul (aparece 4 vezes)

#### Caracter√≠sticas
- ‚úÖ √ötil para dados **categ√≥ricos**
- ‚úÖ N√£o afetada por outliers
- ‚ùå Pode n√£o existir ou haver m√∫ltiplas modas

#### Em IA
A moda √© usada para:
- **Classifica√ß√£o**: Prever categoria mais comum
- **Imputa√ß√£o**: Preencher valores faltantes com a moda
- **An√°lise de features categ√≥ricas**

## Quando Usar Cada Uma?

### Use M√©dia quando:
- ‚úÖ Dados s√£o **sim√©tricos** (distribui√ß√£o normal)
- ‚úÖ **Sem outliers** significativos
- ‚úÖ Precisa de uma medida que use todos os dados

### Use Mediana quando:
- ‚úÖ Dados t√™m **outliers**
- ‚úÖ Distribui√ß√£o √© **assim√©trica**
- ‚úÖ Precisa de uma medida **robusta**

### Use Moda quando:
- ‚úÖ Dados s√£o **categ√≥ricos** (cores, categorias)
- ‚úÖ Quer saber o valor **mais comum**
- ‚úÖ Dados nominais (sem ordem)

## Visualiza√ß√£o

Veja o diagrama abaixo para visualizar como m√©dia, mediana e desvio padr√£o se relacionam com os dados!
          `,
          examples: [
            {
              title: 'Exemplo: Dados [2, 4, 4, 6, 8]',
              description: 'Calcule m√©dia, mediana e moda',
              solution: `
- M√©dia: (2+4+4+6+8)/5 = 24/5 = 4.8
- Mediana: 4 (valor do meio)
- Moda: 4 (aparece 2 vezes)
              `,
            },
          ],
          visualizations: [
            {
              type: 'interactive',
              component: 'statistics',
              data: { showMean: true, showMedian: true },
            },
          ],
        },
        exercises: [
          {
            id: 'ex1',
            question: 'Qual √© a m√©dia de [10, 20, 30, 40]?',
            type: 'input',
            correctAnswer: '25',
            explanation: '(10+20+30+40)/4 = 100/4 = 25',
          },
          {
            id: 'ex2',
            question: 'Qual √© a mediana de [1, 3, 5, 7, 9]?',
            type: 'multiple-choice',
            options: ['3', '5', '7', '4'],
            correctAnswer: '5',
            explanation: 'O valor central quando ordenado √© 5',
          },
          {
            id: 'ex3',
            question: 'Qual √© a moda de [2, 3, 3, 4, 4, 4, 5]?',
            type: 'input',
            correctAnswer: '4',
            explanation: 'O valor 4 aparece 3 vezes, mais que qualquer outro',
          },
        ],
      },
      {
        id: 'desvio-padrao',
        title: 'Desvio Padr√£o',
        description: 'Entenda variabilidade e desvio padr√£o',
        content: {
          theory: `
# Desvio Padr√£o

## O que √© Desvio Padr√£o?

Imagine duas turmas de alunos com a **mesma m√©dia** de nota (7.0):
- **Turma A**: [6.8, 7.0, 7.2, 6.9, 7.1] - notas muito pr√≥ximas
- **Turma B**: [3.0, 7.0, 10.0, 5.0, 10.0] - notas muito espalhadas

Ambas t√™m m√©dia 7.0, mas s√£o **completamente diferentes**! O desvio padr√£o mede essa **variabilidade**.

## Defini√ß√£o

O **desvio padr√£o** (œÉ) mede o quanto os dados se **espalham** em rela√ß√£o √† m√©dia.

### F√≥rmula

**œÉ = ‚àö(Œ£(x·µ¢ - Œº)¬≤ / n)**

Onde:
- **œÉ**: Desvio padr√£o
- **x·µ¢**: Cada valor individual
- **Œº**: M√©dia dos valores
- **n**: N√∫mero de valores

### Passo a Passo

1. Calcule a **m√©dia** (Œº)
2. Para cada valor, calcule a **diferen√ßa** da m√©dia: (x·µ¢ - Œº)
3. **Eleve ao quadrado**: (x·µ¢ - Œº)¬≤ (remove sinais negativos)
4. **Some todos**: Œ£(x·µ¢ - Œº)¬≤
5. **Divida por n**: Œ£(x·µ¢ - Œº)¬≤ / n (vari√¢ncia)
6. **Tire a raiz quadrada**: ‚àö(vari√¢ncia) = desvio padr√£o

## Interpreta√ß√£o Visual

Veja o diagrama abaixo! O desvio padr√£o mostra:
- **Linhas roxas**: ¬±1 desvio padr√£o da m√©dia
- **Dados dentro das linhas**: ~68% dos dados (regra emp√≠rica)
- **Desvio pequeno**: Dados concentrados (curva estreita)
- **Desvio grande**: Dados espalhados (curva larga)

## Regra Emp√≠rica (68-95-99.7)

Para dados com distribui√ß√£o normal:
- **68%** dos dados est√£o dentro de ¬±1 desvio padr√£o
- **95%** dos dados est√£o dentro de ¬±2 desvios padr√£o
- **99.7%** dos dados est√£o dentro de ¬±3 desvios padr√£o

## Por que √© ESSENCIAL em IA?

### 1. Normaliza√ß√£o de Dados

Antes de treinar modelos, voc√™ **normaliza** os dados:
- **Z-score**: (x - Œº) / œÉ
- Isso coloca todos os features na mesma escala
- **Cr√≠tico** para redes neurais!

### 2. Feature Scaling

Algoritmos como SVM, KNN, e redes neurais s√£o sens√≠veis √† escala:
- Features com valores grandes dominam
- Normaliza√ß√£o resolve isso usando desvio padr√£o

### 3. Detec√ß√£o de Outliers

Valores que est√£o a mais de **3 desvios padr√£o** da m√©dia s√£o considerados outliers:
- √ötil para limpeza de dados
- Preven√ß√£o de overfitting

### 4. An√°lise Explorat√≥ria

O desvio padr√£o te diz:
- Se os dados s√£o **consistentes** ou **vari√°veis**
- Se precisa de mais dados
- Qual a confian√ßa nas suas m√©tricas

### 5. Regulariza√ß√£o

Em modelos de ML, o desvio padr√£o ajuda a:
- **L2 Regularization**: Penaliza pesos grandes (relacionado √† vari√¢ncia)
- **Dropout**: Reduz vari√¢ncia em redes neurais

## Exemplo Pr√°tico em ML

**Dataset de pre√ßos de casas**:
- M√©dia: R$ 500.000
- Desvio padr√£o: R$ 100.000

**Interpreta√ß√£o**:
- 68% das casas custam entre R$ 400.000 e R$ 600.000
- Uma casa de R$ 800.000 est√° a 3 desvios (poss√≠vel outlier)

Veja o diagrama visual para entender melhor!
          `,
          examples: [
            {
              title: 'Exemplo: Calcule o desvio padr√£o de [2, 4, 4, 4, 5, 5, 7, 9]',
              description: 'Passo a passo',
              solution: `
1. M√©dia: (2+4+4+4+5+5+7+9)/8 = 40/8 = 5
2. Diferen√ßas: [-3, -1, -1, -1, 0, 0, 2, 4]
3. Quadrados: [9, 1, 1, 1, 0, 0, 4, 16]
4. M√©dia dos quadrados: 32/8 = 4
5. Desvio padr√£o: ‚àö4 = 2
              `,
            },
          ],
        },
        exercises: [
          {
            id: 'ex1',
            question: 'Se a m√©dia √© 10 e os valores s√£o [8, 10, 12], qual √© aproximadamente o desvio padr√£o?',
            type: 'multiple-choice',
            options: ['1.63', '2', '1', '1.5'],
            correctAnswer: '1.63',
            explanation: 'Diferen√ßas: [-2, 0, 2]. Quadrados: [4, 0, 4]. M√©dia: 8/3 ‚âà 2.67. ‚àö2.67 ‚âà 1.63',
          },
          {
            id: 'ex2',
            question: 'Se o desvio padr√£o √© grande, os dados s√£o:',
            type: 'multiple-choice',
            options: [
              'Muito espalhados',
              'Pr√≥ximos da m√©dia',
              'Todos iguais',
              'Imposs√≠vel determinar',
            ],
            correctAnswer: 'Muito espalhados',
            explanation: 'Desvio padr√£o grande indica alta variabilidade',
          },
        ],
      },
      {
        id: 'correlacao',
        title: 'Correla√ß√£o',
        description: 'Entenda como vari√°veis se relacionam',
        content: {
          theory: `
# Correla√ß√£o

## O que √© Correla√ß√£o?

Imagine que voc√™ observa:
- Quando a **temperatura** aumenta, as vendas de **sorvete** tamb√©m aumentam
- Quando o **pre√ßo** de um produto aumenta, as **vendas** diminuem

Essas s√£o **correla√ß√µes** - rela√ß√µes entre duas vari√°veis!

A **correla√ß√£o** mede a **for√ßa e dire√ß√£o** da rela√ß√£o linear entre duas vari√°veis.

## Coeficiente de Correla√ß√£o (r)

O coeficiente de correla√ß√£o de Pearson varia de **-1 a +1**:

### r = +1: Correla√ß√£o Positiva Perfeita
- Quando uma vari√°vel **aumenta**, a outra **aumenta** proporcionalmente
- **Exemplo**: Altura e peso (geralmente pessoas mais altas pesam mais)
- **Gr√°fico**: Linha reta subindo da esquerda para direita

### r = 0: Sem Correla√ß√£o
- N√£o h√° rela√ß√£o linear entre as vari√°veis
- **Exemplo**: N√∫mero de sapatos e QI (n√£o relacionado)
- **Gr√°fico**: Nuvem de pontos sem padr√£o

### r = -1: Correla√ß√£o Negativa Perfeita
- Quando uma vari√°vel **aumenta**, a outra **diminui** proporcionalmente
- **Exemplo**: Pre√ßo e demanda (quanto mais caro, menos vendas)
- **Gr√°fico**: Linha reta descendo da esquerda para direita

## Interpreta√ß√£o da For√ßa

### Correla√ß√£o Forte: |r| > 0.7
- Rela√ß√£o muito clara entre as vari√°veis
- **Exemplo**: r = 0.85 entre horas de estudo e nota

### Correla√ß√£o Moderada: 0.3 < |r| < 0.7
- Rela√ß√£o percept√≠vel, mas n√£o perfeita
- **Exemplo**: r = 0.5 entre exerc√≠cio e perda de peso

### Correla√ß√£o Fraca: |r| < 0.3
- Rela√ß√£o muito fraca ou inexistente
- **Exemplo**: r = 0.15 entre cor do cabelo e intelig√™ncia

## ‚ö†Ô∏è AVISO IMPORTANTE

### Correla√ß√£o ‚â† Causalidade!

**Correla√ß√£o n√£o implica causalidade!** Duas vari√°veis podem estar correlacionadas sem que uma cause a outra.

#### Exemplos Cl√°ssicos

1. **Correla√ß√£o Esp√∫ria**:
   - Vendas de sorvete e afogamentos est√£o correlacionados
   - Mas n√£o √© porque sorvete causa afogamentos!
   - A **causa real** √© o ver√£o (terceira vari√°vel)

2. **Correla√ß√£o Reversa**:
   - Mais m√©dicos em uma cidade ‚Üí Mais doen√ßas?
   - Na verdade, mais doen√ßas ‚Üí mais m√©dicos!

3. **Coincid√™ncia**:
   - N√∫mero de filmes de Nicolas Cage e afogamentos em piscinas
   - Correla√ß√£o alta, mas completamente aleat√≥ria!

## Por que √© CRUCIAL em IA?

### 1. Feature Selection (Sele√ß√£o de Features)

Antes de treinar um modelo:
- **Remova features** altamente correlacionadas (redundantes)
- **Mantenha features** com alta correla√ß√£o com o target
- **Reduz overfitting** e melhora performance

### 2. Detec√ß√£o de Multicolinearidade

Em regress√£o linear:
- Features muito correlacionadas causam **instabilidade**
- Coeficientes ficam imprecisos
- **Solu√ß√£o**: Remover ou combinar features correlacionadas

### 3. An√°lise Explorat√≥ria de Dados (EDA)

A correla√ß√£o ajuda a:
- **Entender** rela√ß√µes nos dados
- **Identificar** padr√µes interessantes
- **Priorizar** features para an√°lise

### 4. Redu√ß√£o de Dimensionalidade

- **PCA** (Principal Component Analysis) usa correla√ß√£o
- Encontra combina√ß√µes de features n√£o correlacionadas
- Reduz n√∫mero de features mantendo informa√ß√£o

### 5. Feature Engineering

Criar novas features baseadas em correla√ß√µes:
- **Intera√ß√µes**: Se X e Y s√£o correlacionados, criar X√óY
- **Ratios**: Se h√° correla√ß√£o, criar raz√µes entre features

## Matriz de Correla√ß√£o

Em datasets com muitas features, voc√™ cria uma **matriz de correla√ß√£o**:
- Mostra correla√ß√£o entre **todos os pares** de features
- Visualiza√ß√£o em heatmap (mapa de calor)
- Identifica rapidamente features redundantes

## Exemplo Pr√°tico

**Dataset de casas**:
- √Årea e pre√ßo: r = 0.85 (forte correla√ß√£o positiva)
- N√∫mero de quartos e pre√ßo: r = 0.72 (forte correla√ß√£o positiva)
- √Årea e n√∫mero de quartos: r = 0.65 (moderada - podem ser redundantes)

**A√ß√£o**: Considerar remover uma das features (√°rea ou quartos) para evitar multicolinearidade.

A correla√ß√£o √© uma das ferramentas mais importantes na an√°lise de dados e ML!
          `,
          examples: [
            {
              title: 'Exemplo: Correla√ß√£o Positiva',
              description: 'Altura e peso geralmente t√™m correla√ß√£o positiva',
              solution: 'Pessoas mais altas tendem a pesar mais, ent√£o r > 0',
            },
          ],
        },
        exercises: [
          {
            id: 'ex1',
            question: 'Se duas vari√°veis t√™m correla√ß√£o r = -0.85, isso significa:',
            type: 'multiple-choice',
            options: [
              'Correla√ß√£o negativa forte',
              'Correla√ß√£o positiva forte',
              'Sem correla√ß√£o',
              'Correla√ß√£o moderada',
            ],
            correctAnswer: 'Correla√ß√£o negativa forte',
            explanation: 'r = -0.85 est√° pr√≥ximo de -1, indicando correla√ß√£o negativa forte',
          },
        ],
      },
    ],
  },
  {
    id: 'probabilidade',
    title: 'Probabilidade',
    description: 'Fundamentos de probabilidade para Machine Learning',
    icon: 'üé≤',
    color: 'bg-purple-500',
    lessons: [
      {
        id: 'probabilidade-basica',
        title: 'Probabilidade B√°sica',
        description: 'Entenda o que √© probabilidade e como usar em ML',
        content: {
          theory: `
# Probabilidade B√°sica

## O que √© Probabilidade?

Probabilidade √© uma forma de medir **incerteza**. Em programa√ß√£o, voc√™ j√° usa isso quando faz:

\`\`\`python
import random
resultado = random.choice([0, 1])  # 50% de chance de cada
\`\`\`

Em Machine Learning, **tudo √© probabil√≠stico**! Os modelos n√£o d√£o respostas certas, d√£o **probabilidades**.

## Conceito Simples

**Probabilidade = N√∫mero de casos favor√°veis / N√∫mero total de casos**

**Exemplo**: Qual a probabilidade de tirar "cara" ao jogar uma moeda?
- Casos favor√°veis: 1 (cara)
- Total de casos: 2 (cara ou coroa)
- Probabilidade: 1/2 = 0.5 = 50%

## Em C√≥digo (Python)

\`\`\`python
# Simular probabilidade
import random

def probabilidade_cara():
    resultados = []
    for _ in range(1000):
        resultados.append(random.choice(['cara', 'coroa']))
    
    # Contar quantas vezes deu cara
    caras = resultados.count('cara')
    prob = caras / len(resultados)
    return prob  # Deve ser pr√≥ximo de 0.5

# Em ML, voc√™ usa isso o tempo todo!
# Modelos retornam probabilidades:
# "Esta imagem tem 85% de chance de ser um gato"
\`\`\`

## Por que √© Essencial em IA?

1. **Classifica√ß√£o**: Modelos retornam probabilidades (ex: 80% gato, 20% cachorro)
2. **Naive Bayes**: Algoritmo baseado totalmente em probabilidade
3. **Redes Neurais**: √öltima camada (softmax) retorna probabilidades
4. **Incerteza**: ML lida com incerteza, n√£o certeza absoluta

## Distribui√ß√µes Comuns

### Distribui√ß√£o Uniforme
Todos os valores t√™m a mesma chance:

\`\`\`python
# Dado de 6 lados: cada face tem 1/6 de chance
import numpy as np
np.random.uniform(1, 7)  # N√∫mero entre 1 e 6
\`\`\`

### Distribui√ß√£o Normal (Gaussiana)
A maioria dos valores fica perto da m√©dia (curva em sino):

\`\`\`python
# Altura das pessoas, erros de medi√ß√£o, etc
import numpy as np
dados = np.random.normal(170, 10, 1000)  # M√©dia=170, Desvio=10
# A maioria fica entre 160-180
\`\`\`

## Probabilidade Condicional

**P(A|B)** = Probabilidade de A **dado que** B aconteceu

**Exemplo Pr√°tico em ML**:
- P(Spam | palavra="gratis") = Probabilidade de ser spam SE cont√©m "gratis"
- Isso √© usado em Naive Bayes!

\`\`\`python
# Exemplo conceitual
# Se um email tem "gratis", qual a chance de ser spam?
# P(spam | "gratis") = P("gratis" | spam) * P(spam) / P("gratis")
\`\`\`

## Aplica√ß√£o Pr√°tica: Naive Bayes

\`\`\`python
from sklearn.naive_bayes import MultinomialNB

# Classificador de spam (usa probabilidade!)
modelo = MultinomialNB()
modelo.fit(X_treino, y_treino)

# Retorna probabilidades
probabilidades = modelo.predict_proba(X_teste)
# [[0.1, 0.9], [0.8, 0.2], ...]
# [probabilidade n√£o-spam, probabilidade spam]
\`\`\`

## Resumo para Programadores

- **Probabilidade** = Medir incerteza (0 a 1, ou 0% a 100%)
- Em ML, modelos retornam **probabilidades**, n√£o certezas
- Voc√™ j√° usa probabilidade quando faz \`random.choice()\`
- Naive Bayes, Softmax, e muitos algoritmos usam probabilidade
- **N√£o precisa calcular manualmente!** Bibliotecas fazem isso
          `,
          examples: [
            {
              title: 'Exemplo: Simular Probabilidade',
              description: 'Simule 1000 lan√ßamentos de moeda',
              solution: `import random

resultados = [random.choice(['cara', 'coroa']) for _ in range(1000)]
prob_cara = resultados.count('cara') / len(resultados)
print(f"Probabilidade de cara: {prob_cara:.2%}")  # ~50%`,
            },
          ],
        },
        exercises: [
          {
            id: 'ex1',
            question: 'Se um modelo de ML retorna [0.2, 0.8] para uma imagem, isso significa:',
            type: 'multiple-choice',
            options: [
              '20% de chance classe 1, 80% de chance classe 2',
              '20% de certeza classe 1, 80% de certeza classe 2',
              'A imagem tem 80 pixels',
              'O modelo tem 80% de acur√°cia',
            ],
            correctAnswer: '20% de chance classe 1, 80% de chance classe 2',
            explanation: 'Modelos retornam probabilidades, n√£o certezas absolutas',
          },
          {
            id: 'ex2',
            question: 'Qual a probabilidade de tirar um n√∫mero par ao jogar um dado?',
            type: 'input',
            correctAnswer: '0.5',
            explanation: 'N√∫meros pares: 2, 4, 6 (3 casos). Total: 6. Probabilidade = 3/6 = 0.5',
          },
        ],
      },
    ],
  },
  {
    id: 'machine-learning',
    title: 'Machine Learning Pr√°tico',
    description: 'Conceitos pr√°ticos essenciais para trabalhar com ML',
    icon: 'ü§ñ',
    color: 'bg-indigo-500',
    lessons: [
      {
        id: 'funcoes-custo',
        title: 'Fun√ß√µes de Custo/Perda',
        description: 'Entenda como modelos aprendem: minimizando o erro',
        content: {
          theory: `
# Fun√ß√µes de Custo/Perda (Loss Functions)

## O que √© uma Fun√ß√£o de Custo?

√â uma **medida de erro**. O modelo tenta **minimizar** essa fun√ß√£o para aprender.

Pense assim: o modelo est√° "tentando acertar" e a fun√ß√£o de custo mede "qu√£o errado ele est√°".

## Analogia Simples

Imagine que voc√™ est√° jogando dardos:
- **Fun√ß√£o de custo** = qu√£o longe voc√™ est√° do alvo
- **Treinar o modelo** = ajustar sua mira para acertar mais perto
- **Custo baixo** = acertou perto do alvo! üéØ
- **Custo alto** = errou feio! ‚ùå

## Por que √© Essencial?

**O modelo aprende minimizando o custo!** √â assim que funciona:

1. Modelo faz uma previs√£o
2. Calcula o erro (fun√ß√£o de custo)
3. Ajusta os pesos para reduzir o erro
4. Repete at√© o erro ficar pequeno

## MSE - Mean Squared Error (Regress√£o)

**Quando usar**: Prever n√∫meros cont√≠nuos (pre√ßo, temperatura, etc)

\`\`\`python
from sklearn.metrics import mean_squared_error

# Exemplo: Prever pre√ßo de casas
y_real = [100, 200, 300]      # Pre√ßos reais
y_predito = [110, 190, 310]   # Pre√ßos que o modelo previu

mse = mean_squared_error(y_real, y_predito)
# MSE = m√©dia dos erros ao quadrado
# Quanto menor, melhor!
\`\`\`

**F√≥rmula**: MSE = (1/n) √ó Œ£(y_real - y_predito)¬≤

**Por que ao quadrado?** Para penalizar erros grandes mais!

## Cross-Entropy (Classifica√ß√£o)

**Quando usar**: Classificar em categorias (gato/cachorro, spam/n√£o-spam)

\`\`\`python
from sklearn.metrics import log_loss

# Exemplo: Classificar imagens
y_real = [1, 0, 1]  # 1=gato, 0=cachorro
y_predito = [0.9, 0.2, 0.8]  # Probabilidades

cross_entropy = log_loss(y_real, y_predito)
# Quanto menor, melhor!
\`\`\`

**Por que usar?** Penaliza muito quando o modelo est√° "confiante mas errado"

## Em Redes Neurais (TensorFlow/Keras)

\`\`\`python
from tensorflow import keras

modelo = keras.Sequential([
    keras.layers.Dense(10, activation='relu'),
    keras.layers.Dense(1)  # Sa√≠da
])

# Escolha a fun√ß√£o de custo baseado no problema:
# Regress√£o ‚Üí MSE
modelo.compile(optimizer='adam', loss='mse')

# Classifica√ß√£o ‚Üí Cross-entropy
modelo.compile(optimizer='adam', loss='binary_crossentropy')
# ou
modelo.compile(optimizer='adam', loss='categorical_crossentropy')
\`\`\`

## Resumo Pr√°tico

| Problema | Fun√ß√£o de Custo | C√≥digo |
|----------|----------------|--------|
| Prever n√∫mero (regress√£o) | MSE | \`loss='mse'\` |
| Classificar 2 classes | Binary Cross-Entropy | \`loss='binary_crossentropy'\` |
| Classificar v√°rias classes | Categorical Cross-Entropy | \`loss='categorical_crossentropy'\` |

**Dica**: Voc√™ n√£o precisa implementar! Escolha a fun√ß√£o certa e a biblioteca faz o resto.

## Visualiza√ß√£o do Aprendizado

Durante o treinamento, voc√™ v√™ o custo diminuindo:

\`\`\`python
# Ao treinar, voc√™ v√™:
# √âpoca 1: loss = 0.5
# √âpoca 2: loss = 0.3
# √âpoca 3: loss = 0.2
# ...
# √âpoca 100: loss = 0.01  ‚Üê Modelo aprendeu!
\`\`\`

**Custo diminuindo = Modelo aprendendo!** üìâ
          `,
          examples: [
            {
              title: 'Exemplo: MSE em Regress√£o',
              description: 'Calcule o MSE para previs√µes de pre√ßo',
              solution: `from sklearn.metrics import mean_squared_error

precos_reais = [100000, 200000, 300000]
precos_preditos = [105000, 195000, 310000]

mse = mean_squared_error(precos_reais, precos_preditos)
print(f"Erro m√©dio: {mse:.0f}")  # Quanto menor, melhor!`,
            },
          ],
        },
        exercises: [
          {
            id: 'ex1',
            question: 'Para prever o pre√ßo de uma casa, qual fun√ß√£o de custo usar?',
            type: 'multiple-choice',
            options: [
              'MSE (Mean Squared Error)',
              'Cross-Entropy',
              'Accuracy',
              'Precision',
            ],
            correctAnswer: 'MSE (Mean Squared Error)',
            explanation: 'MSE √© usado para regress√£o (prever n√∫meros cont√≠nuos como pre√ßos)',
          },
          {
            id: 'ex2',
            question: 'O que significa quando a fun√ß√£o de custo diminui durante o treinamento?',
            type: 'multiple-choice',
            options: [
              'O modelo est√° aprendendo (erro diminuindo)',
              'O modelo est√° piorando',
              'O modelo parou de aprender',
              'Houve um erro no c√≥digo',
            ],
            correctAnswer: 'O modelo est√° aprendendo (erro diminuindo)',
            explanation: 'Custo = erro. Custo diminuindo = erro diminuindo = modelo melhorando!',
          },
        ],
      },
      {
        id: 'overfitting-underfitting',
        title: 'Overfitting e Underfitting',
        description: 'Os problemas mais comuns em ML e como resolver',
        content: {
          theory: `
# Overfitting e Underfitting

## O Problema Mais Comum em ML

Esses s√£o os **dois maiores problemas** que voc√™ vai enfrentar. Entender isso √© **essencial**!

## Underfitting (Subajuste)

**O que √©**: Modelo muito simples, n√£o aprende nem os dados de treino.

**Sintomas**:
- Erro alto no treino
- Erro alto no teste
- Modelo "n√£o entendeu" os dados

**Analogia**: Como um estudante que n√£o estudou nada e vai mal em tudo.

\`\`\`python
# Exemplo: Modelo muito simples para dados complexos
from sklearn.linear_model import LinearRegression

# Dados complexos (n√£o-lineares)
X = [[1], [2], [3], [4], [5]]
y = [1, 4, 9, 16, 25]  # y = x¬≤ (curva!)

modelo = LinearRegression()  # Muito simples!
modelo.fit(X, y)
# Vai ter erro alto - modelo linear n√£o captura curva
\`\`\`

**Solu√ß√£o**: Usar modelo mais complexo (mais camadas, mais features, etc)

## Overfitting (Sobreajuste)

**O que √©**: Modelo muito complexo, "decorou" os dados de treino mas n√£o generaliza.

**Sintomas**:
- Erro **baixo** no treino (parece perfeito!)
- Erro **alto** no teste (n√£o funciona com dados novos)
- Modelo "decorou" ao inv√©s de "aprender"

**Analogia**: Como um estudante que decorou tudo mas n√£o entendeu - vai mal em provas novas.

\`\`\`python
# Exemplo visual do problema:
# Treino: accuracy = 99%  ‚Üê Parece √≥timo!
# Teste: accuracy = 60%    ‚Üê Mas n√£o funciona com dados novos!
\`\`\`

**Solu√ß√£o**: Regulariza√ß√£o, mais dados, modelo mais simples

## Como Identificar?

\`\`\`python
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Dividir dados
X_treino, X_teste, y_treino, y_teste = train_test_split(X, y, test_size=0.2)

# Treinar
modelo.fit(X_treino, y_treino)

# Avaliar
acc_treino = accuracy_score(y_treino, modelo.predict(X_treino))
acc_teste = accuracy_score(y_teste, modelo.predict(X_teste))

print(f"Treino: {acc_treino:.2%}")
print(f"Teste: {acc_teste:.2%}")

# Se acc_treino >> acc_teste ‚Üí OVERFITTING!
# Se ambos baixos ‚Üí UNDERFITTING!
\`\`\`

## Solu√ß√µes Pr√°ticas

### Para Overfitting:

1. **Regulariza√ß√£o** (L1, L2)
\`\`\`python
from sklearn.linear_model import Ridge  # L2
modelo = Ridge(alpha=0.1)  # alpha = for√ßa da regulariza√ß√£o
\`\`\`

2. **Dropout** (em redes neurais)
\`\`\`python
from tensorflow import keras
keras.layers.Dropout(0.5)  # Desativa 50% dos neur√¥nios aleatoriamente
\`\`\`

3. **Mais dados** (sempre ajuda!)

4. **Early Stopping** (parar antes de decorar)
\`\`\`python
from tensorflow import keras
modelo.fit(X, y, validation_split=0.2, epochs=100,
           callbacks=[keras.callbacks.EarlyStopping(patience=5)])
# Para se n√£o melhorar por 5 √©pocas
\`\`\`

### Para Underfitting:

1. **Modelo mais complexo**
2. **Mais features** (mais informa√ß√µes)
3. **Treinar por mais tempo**

## O Equil√≠brio Perfeito

**Ideal**: Erro de treino e teste similares e baixos!

\`\`\`python
# Bom modelo:
# Treino: accuracy = 85%
# Teste: accuracy = 83%  ‚Üê Pr√≥ximos! Generaliza bem!
\`\`\`

## Resumo Visual

- **Underfitting**: Modelo muito simples, n√£o captura padr√£o
- **Overfitting**: Modelo muito complexo, decorou os dados
- **Ideal**: Modelo que captura padr√£o e generaliza

## Dica de Ouro

**Sempre compare treino vs teste!** Se a diferen√ßa for grande, voc√™ tem um problema.
          `,
          examples: [
            {
              title: 'Exemplo: Detectar Overfitting',
              description: 'Compare acur√°cia de treino e teste',
              solution: `from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Dividir dados
X_treino, X_teste, y_treino, y_teste = train_test_split(X, y)

# Treinar e avaliar
modelo.fit(X_treino, y_treino)
acc_treino = accuracy_score(y_treino, modelo.predict(X_treino))
acc_teste = accuracy_score(y_teste, modelo.predict(X_teste))

# Se acc_treino muito maior que acc_teste ‚Üí Overfitting!
if acc_treino - acc_teste > 0.2:
    print("ATEN√á√ÉO: Poss√≠vel overfitting!")`,
            },
          ],
        },
        exercises: [
          {
            id: 'ex1',
            question: 'Um modelo tem accuracy de 99% no treino e 60% no teste. Isso indica:',
            type: 'multiple-choice',
            options: [
              'Overfitting (decorou os dados)',
              'Underfitting (modelo muito simples)',
              'Modelo perfeito',
              'Erro no c√≥digo',
            ],
            correctAnswer: 'Overfitting (decorou os dados)',
            explanation: 'Grande diferen√ßa entre treino e teste indica overfitting',
          },
          {
            id: 'ex2',
            question: 'Qual t√©cnica ajuda a reduzir overfitting?',
            type: 'multiple-choice',
            options: [
              'Regulariza√ß√£o (L1/L2)',
              'Usar modelo mais complexo',
              'Treinar por mais tempo',
              'Usar menos dados',
            ],
            correctAnswer: 'Regulariza√ß√£o (L1/L2)',
            explanation: 'Regulariza√ß√£o penaliza modelos muito complexos, reduzindo overfitting',
          },
        ],
      },
      {
        id: 'metricas-avaliacao',
        title: 'M√©tricas de Avalia√ß√£o',
        description: 'Como medir se seu modelo est√° bom',
        content: {
          theory: `
# M√©tricas de Avalia√ß√£o

## Por que Precisamos de M√©tricas?

Voc√™ precisa saber: **seu modelo est√° bom ou ruim?** M√©tricas te d√£o essa resposta!

## Accuracy (Acur√°cia) - A Mais Simples

**Quando usar**: Problemas balanceados (classes com quantidades similares)

\`\`\`python
from sklearn.metrics import accuracy_score

y_real = [1, 0, 1, 1, 0]
y_predito = [1, 0, 1, 0, 0]

accuracy = accuracy_score(y_real, y_predito)
# 4 de 5 corretos = 0.8 = 80%
\`\`\`

**Problema**: N√£o funciona bem quando classes s√£o desbalanceadas!

**Exemplo do problema**:
- 1000 emails: 990 n√£o-spam, 10 spam
- Modelo que sempre diz "n√£o-spam" tem 99% accuracy!
- Mas √© in√∫til (nunca detecta spam)!

## Precision (Precis√£o)

**Pergunta**: Das vezes que o modelo disse "spam", quantas eram realmente spam?

**Quando usar**: Quando falsos positivos s√£o caros (ex: marcar email importante como spam)

\`\`\`python
from sklearn.metrics import precision_score

precision = precision_score(y_real, y_predito)
# Das previs√µes positivas, quantas eram realmente positivas?
\`\`\`

## Recall (Revoca√ß√£o/Sensibilidade)

**Pergunta**: De todos os spams reais, quantos o modelo encontrou?

**Quando usar**: Quando falsos negativos s√£o caros (ex: deixar spam passar)

\`\`\`python
from sklearn.metrics import recall_score

recall = recall_score(y_real, y_predito)
# Dos casos positivos reais, quantos foram encontrados?
\`\`\`

## F1-Score - O Equil√≠brio

**Combina Precision e Recall** em uma √∫nica m√©trica.

**Quando usar**: Quando precisa balancear precision e recall

\`\`\`python
from sklearn.metrics import f1_score

f1 = f1_score(y_real, y_predito)
# M√©dia harm√¥nica de precision e recall
\`\`\`

## Matriz de Confus√£o - Visualizar Tudo

**A melhor forma de entender seu modelo!**

\`\`\`python
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_real, y_predito)
sns.heatmap(cm, annot=True, fmt='d')
plt.show()

# Mostra:
#          Predito
#        0    1
# Real 0 [TN  FP]
#     1 [FN  TP]
\`\`\`

**O que significa**:
- **TP (True Positive)**: Acertou positivo
- **TN (True Negative)**: Acertou negativo
- **FP (False Positive)**: Errou (disse positivo mas era negativo)
- **FN (False Negative)**: Errou (disse negativo mas era positivo)

## Qual M√©trica Usar?

| Situa√ß√£o | M√©trica |
|----------|---------|
| Classes balanceadas | Accuracy |
| Falsos positivos s√£o caros | Precision |
| Falsos negativos s√£o caros | Recall |
| Precisa balancear ambos | F1-Score |
| Quer ver tudo | Matriz de Confus√£o |

## Exemplo Pr√°tico: Detec√ß√£o de Spam

\`\`\`python
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Resultados do modelo
y_real = [1, 1, 0, 1, 0, 0, 1, 0]  # 1=spam, 0=n√£o-spam
y_predito = [1, 1, 0, 0, 0, 1, 1, 0]

print(f"Accuracy: {accuracy_score(y_real, y_predito):.2%}")
print(f"Precision: {precision_score(y_real, y_predito):.2%}")
print(f"Recall: {recall_score(y_real, y_predito):.2%}")
print(f"F1: {f1_score(y_real, y_predito):.2%}")

# Interpreta√ß√£o:
# - Accuracy: 75% dos emails classificados corretamente
# - Precision: Das vezes que disse spam, 75% eram realmente spam
# - Recall: Dos spams reais, encontrou 75%
# - F1: Balanceamento entre precision e recall
\`\`\`

## Para Regress√£o (Prever N√∫meros)

\`\`\`python
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# MSE (j√° vimos)
mse = mean_squared_error(y_real, y_predito)

# MAE (mais f√°cil de interpretar)
mae = mean_absolute_error(y_real, y_predito)
# Erro m√©dio absoluto: "em m√©dia, erra X unidades"

# R¬≤ (quanto do erro explica)
r2 = r2_score(y_real, y_predito)
# 1.0 = perfeito, 0.0 = n√£o explica nada
\`\`\`

## Resumo para Programadores

- **Accuracy**: Simples, mas cuidado com classes desbalanceadas
- **Precision/Recall**: Use quando falsos positivos/negativos importam
- **F1**: Bom equil√≠brio geral
- **Matriz de Confus√£o**: Sempre olhe! Mostra tudo
- **Para regress√£o**: MSE, MAE, R¬≤

**Dica**: Comece sempre com matriz de confus√£o para entender o modelo!
          `,
          examples: [
            {
              title: 'Exemplo: Calcular Todas as M√©tricas',
              description: 'Avalie um modelo de classifica√ß√£o',
              solution: `from sklearn.metrics import (accuracy_score, precision_score, 
                          recall_score, f1_score, confusion_matrix)

y_real = [1, 1, 0, 1, 0]
y_predito = [1, 0, 0, 1, 1]

print(f"Accuracy: {accuracy_score(y_real, y_predito):.2%}")
print(f"Precision: {precision_score(y_real, y_predito):.2%}")
print(f"Recall: {recall_score(y_real, y_predito):.2%}")
print(f"F1: {f1_score(y_real, y_predito):.2%}")
print(f"Matriz de Confus√£o:\\n{confusion_matrix(y_real, y_predito)}")`,
            },
          ],
        },
        exercises: [
          {
            id: 'ex1',
            question: 'Para um problema onde falsos negativos s√£o muito caros (ex: detectar c√¢ncer), qual m√©trica priorizar?',
            type: 'multiple-choice',
            options: [
              'Recall (encontrar todos os casos positivos)',
              'Precision (evitar falsos positivos)',
              'Accuracy',
              'F1-Score',
            ],
            correctAnswer: 'Recall (encontrar todos os casos positivos)',
            explanation: 'Recall mede quantos casos positivos foram encontrados. Importante quando n√£o pode deixar passar casos positivos',
          },
          {
            id: 'ex2',
            question: 'O que mostra uma Matriz de Confus√£o?',
            type: 'multiple-choice',
            options: [
              'TP, TN, FP, FN - todos os tipos de acertos e erros',
              'Apenas os acertos',
              'Apenas os erros',
              'A complexidade do modelo',
            ],
            correctAnswer: 'TP, TN, FP, FN - todos os tipos de acertos e erros',
            explanation: 'Matriz de confus√£o mostra todos os 4 casos: verdadeiros positivos/negativos e falsos positivos/negativos',
          },
        ],
      },
      {
        id: 'normalizacao-standardizacao',
        title: 'Normaliza√ß√£o e Standardiza√ß√£o',
        description: 'Por que e como normalizar dados antes de treinar',
        content: {
          theory: `
# Normaliza√ß√£o e Standardiza√ß√£o

## Por que Normalizar?

**Muitos algoritmos de ML funcionam melhor com dados normalizados!**

**Problema**: Features com escalas diferentes confundem o modelo.

**Exemplo do problema**:
\`\`\`python
# Dados de casas
area = [50, 100, 150]      # metros¬≤ (valores pequenos)
preco = [50000, 100000, 150000]  # reais (valores grandes)

# O modelo vai dar mais peso para "preco" s√≥ porque os n√∫meros s√£o maiores!
# Mas √°rea tamb√©m √© importante!
\`\`\`

**Solu√ß√£o**: Colocar tudo na mesma escala!

## Normaliza√ß√£o (Min-Max Scaling)

**Transforma valores para o intervalo [0, 1]**

\`\`\`python
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
X_normalizado = scaler.fit_transform(X)

# Antes: [50, 100, 150]
# Depois: [0.0, 0.5, 1.0]  # Tudo entre 0 e 1!
\`\`\`

**F√≥rmula**: (valor - m√≠nimo) / (m√°ximo - m√≠nimo)

**Quando usar**: Quando voc√™ sabe os limites dos dados (ex: 0-255 para imagens)

## Standardiza√ß√£o (Z-score)

**Transforma para m√©dia=0 e desvio padr√£o=1**

\`\`\`python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_padronizado = scaler.fit_transform(X)

# Antes: [50, 100, 150]
# Depois: [-1.22, 0.0, 1.22]  # M√©dia=0, desvio=1
\`\`\`

**F√≥rmula**: (valor - m√©dia) / desvio padr√£o

**Quando usar**: Quando voc√™ n√£o sabe os limites (mais comum!)

## Qual Usar?

| Situa√ß√£o | M√©todo |
|----------|--------|
| Dados com limites conhecidos (0-255) | Normaliza√ß√£o (Min-Max) |
| Dados sem limites claros | Standardiza√ß√£o (Z-score) |
| Redes Neurais | Standardiza√ß√£o (geralmente melhor) |
| KNN, SVM | Standardiza√ß√£o (essencial!) |

## Exemplo Completo

\`\`\`python
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Carregar dados
df = pd.read_csv('casas.csv')
X = df[['area', 'quartos', 'idade']]
y = df['preco']

# IMPORTANTE: Normalizar APENAS os dados de treino!
X_treino, X_teste, y_treino, y_teste = train_test_split(X, y)

# Criar scaler e ajustar nos dados de TREINO
scaler = StandardScaler()
scaler.fit(X_treino)  # Aprende m√©dia e desvio do TREINO

# Transformar treino e teste
X_treino_scaled = scaler.transform(X_treino)
X_teste_scaled = scaler.transform(X_teste)  # Usa mesma escala do treino!

# Agora pode treinar
modelo.fit(X_treino_scaled, y_treino)
\`\`\`

**‚ö†Ô∏è ERRO COMUM**: Normalizar treino e teste juntos! Isso "vaza" informa√ß√£o do teste para o treino.

## Pipeline (Fazer Tudo de Uma Vez)

\`\`\`python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

# Cria pipeline: normaliza ‚Üí treina
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('modelo', LogisticRegression())
])

# Treina tudo de uma vez (normaliza automaticamente)
pipeline.fit(X_treino, y_treino)
pipeline.predict(X_teste)  # Normaliza automaticamente tamb√©m!
\`\`\`

## Quando N√ÉO Normalizar?

- **√Årvores de Decis√£o** (Random Forest, XGBoost): N√£o precisam!
- **Naive Bayes**: Geralmente n√£o precisa
- **Dados j√° na mesma escala**: Desnecess√°rio

## Resumo Pr√°tico

1. **Sempre normalize** para: KNN, SVM, Redes Neurais, Regress√£o Linear
2. **Use StandardScaler** (mais comum)
3. **Normalize APENAS com dados de treino**, depois aplique no teste
4. **Use Pipeline** para facilitar

**Dica**: Se n√£o tiver certeza, normalize! Raramente faz mal.
          `,
          examples: [
            {
              title: 'Exemplo: Normalizar Dados',
              description: 'Use StandardScaler antes de treinar',
              solution: `from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Dividir dados
X_treino, X_teste, y_treino, y_teste = train_test_split(X, y)

# Normalizar
scaler = StandardScaler()
X_treino_scaled = scaler.fit_transform(X_treino)
X_teste_scaled = scaler.transform(X_teste)  # N√£o fit! S√≥ transform!

# Treinar com dados normalizados
modelo.fit(X_treino_scaled, y_treino)`,
            },
          ],
        },
        exercises: [
          {
            id: 'ex1',
            question: 'Por que √© importante normalizar dados antes de treinar modelos como KNN ou SVM?',
            type: 'multiple-choice',
            options: [
              'Features com escalas diferentes confundem o modelo',
              'Para acelerar o treinamento',
              'Para reduzir o tamanho dos dados',
              'Para aumentar a acur√°cia sempre',
            ],
            correctAnswer: 'Features com escalas diferentes confundem o modelo',
            explanation: 'Algoritmos como KNN e SVM s√£o sens√≠veis √† escala. Features grandes dominam features pequenas se n√£o normalizar',
          },
          {
            id: 'ex2',
            question: 'Qual √© o erro comum ao normalizar dados?',
            type: 'multiple-choice',
            options: [
              'Normalizar treino e teste juntos (vaza informa√ß√£o)',
              'Normalizar apenas o treino',
              'Usar StandardScaler ao inv√©s de MinMaxScaler',
              'Normalizar a vari√°vel target (y)',
            ],
            correctAnswer: 'Normalizar treino e teste juntos (vaza informa√ß√£o)',
            explanation: 'Deve normalizar apenas com dados de treino, depois aplicar a mesma transforma√ß√£o no teste',
          },
        ],
      },
    ],
  },
]

